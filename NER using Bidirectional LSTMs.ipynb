{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entity Recognition (NER) using Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Named-entity recognition (NER) is also known as entity identification, entity chunking and entity extraction. The objective is to identify entities like person names, organizations, locations etc. from unstructured text.\n",
    "\n",
    "In this project, we will work with a dataset provided by kaggle. The dataset can be accessed from the kaggle link below:\n",
    "https://www.kaggle.com/abhinavwalia95/entity-annotated-corpus\n",
    "\n",
    "The sample data has already been annotated and each word has been tagged with the relevant POS and NER tags. In this project we will focus on identifying NER tags. However, the approach of identifying POS or NER tags using previous annotated data using Deep Learning remains the same. Just a minor change in the code can be used for the POS tagging instead. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>Thousands</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>have</td>\n",
       "      <td>VBP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>marched</td>\n",
       "      <td>VBN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Sentence #           Word  POS Tag\n",
       "0  Sentence: 1      Thousands  NNS   O\n",
       "1          NaN             of   IN   O\n",
       "2          NaN  demonstrators  NNS   O\n",
       "3          NaN           have  VBP   O\n",
       "4          NaN        marched  VBN   O"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(r'Data/ner_dataset.csv', encoding='unicode_escape')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dictionaries to encode the words and NER tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab_dict(data, token_or_tag='token'):\n",
    "    \n",
    "    if token_or_tag == 'token':\n",
    "        vocab = set(data['Word'])\n",
    "    else:\n",
    "        vocab = set(data['Tag'])\n",
    "        \n",
    "    idx2tok = {idx : tok for idx, tok in  enumerate(vocab)}\n",
    "    tok2idx = {tok : idx for idx, tok in  enumerate(vocab)}\n",
    "    \n",
    "    return idx2tok, tok2idx### Transform columns to aggregate the tokens/tags at a sentence level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index to Tags : \n",
      "\n",
      "{0: 'B-art', 1: 'O', 2: 'I-per', 3: 'B-nat', 4: 'B-tim', 5: 'I-geo', 6: 'B-eve', 7: 'B-geo', 8: 'I-gpe', 9: 'I-org', 10: 'I-tim', 11: 'I-nat', 12: 'B-per', 13: 'I-art', 14: 'I-eve', 15: 'B-org', 16: 'B-gpe'}\n",
      "\n",
      "Tags to Index : \n",
      "\n",
      "{'B-art': 0, 'O': 1, 'I-per': 2, 'B-nat': 3, 'B-tim': 4, 'I-geo': 5, 'B-eve': 6, 'B-geo': 7, 'I-gpe': 8, 'I-org': 9, 'I-tim': 10, 'I-nat': 11, 'B-per': 12, 'I-art': 13, 'I-eve': 14, 'B-org': 15, 'B-gpe': 16}\n"
     ]
    }
   ],
   "source": [
    "idx2tag, tag2idx = get_vocab_dict(data, 'tag')\n",
    "idx2word, word2idx = get_vocab_dict(data, 'token')\n",
    "\n",
    "print(\"Index to Tags : \\n\")\n",
    "print(idx2tag)\n",
    "print(\"\\nTags to Index : \\n\")\n",
    "print(tag2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add columns to convert the words and NER tags to numerically encoded values\n",
    "\n",
    "This has to be done as ML models need data in a numerical format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "      <th>Word_idx</th>\n",
       "      <th>Tag_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>Thousands</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "      <td>17755</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "      <td>29027</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "      <td>34706</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>have</td>\n",
       "      <td>VBP</td>\n",
       "      <td>O</td>\n",
       "      <td>11443</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>marched</td>\n",
       "      <td>VBN</td>\n",
       "      <td>O</td>\n",
       "      <td>30908</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Sentence #           Word  POS Tag  Word_idx  Tag_idx\n",
       "0  Sentence: 1      Thousands  NNS   O     17755        1\n",
       "1  Sentence: 1             of   IN   O     29027        1\n",
       "2  Sentence: 1  demonstrators  NNS   O     34706        1\n",
       "3  Sentence: 1           have  VBP   O     11443        1\n",
       "4  Sentence: 1        marched  VBN   O     30908        1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Word_idx'] = data['Word'].map(word2idx)\n",
    "data['Tag_idx'] = data['Tag'].map(tag2idx)\n",
    "data['Sentence #'].fillna(method='ffill', axis=0, inplace=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform columns to aggregate the tokens/tags at a sentence level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program Files\\Anaconda3\\envs\\Tensorflow\\lib\\site-packages\\ipykernel_launcher.py:1: FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "      <th>Word_idx</th>\n",
       "      <th>Tag_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>[Thousands, of, demonstrators, have, marched, ...</td>\n",
       "      <td>[NNS, IN, NNS, VBP, VBN, IN, NNP, TO, VB, DT, ...</td>\n",
       "      <td>[O, O, O, O, O, O, B-geo, O, O, O, O, O, B-geo...</td>\n",
       "      <td>[17755, 29027, 34706, 11443, 30908, 10389, 334...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 7, 1, 1, 1, 1, 1, 7, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sentence: 10</td>\n",
       "      <td>[Iranian, officials, say, they, expect, to, ge...</td>\n",
       "      <td>[JJ, NNS, VBP, PRP, VBP, TO, VB, NN, TO, JJ, J...</td>\n",
       "      <td>[B-gpe, O, O, O, O, O, O, O, O, O, O, O, O, O,...</td>\n",
       "      <td>[26574, 19104, 22117, 25087, 22100, 12616, 219...</td>\n",
       "      <td>[16, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sentence: 100</td>\n",
       "      <td>[Helicopter, gunships, Saturday, pounded, mili...</td>\n",
       "      <td>[NN, NNS, NNP, VBD, JJ, NNS, IN, DT, NNP, JJ, ...</td>\n",
       "      <td>[O, O, B-tim, O, O, O, O, O, B-geo, O, O, O, O...</td>\n",
       "      <td>[8087, 34712, 15377, 2082, 32462, 24997, 13078...</td>\n",
       "      <td>[1, 1, 4, 1, 1, 1, 1, 1, 7, 1, 1, 1, 1, 1, 15,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sentence: 1000</td>\n",
       "      <td>[They, left, after, a, tense, hour-long, stand...</td>\n",
       "      <td>[PRP, VBD, IN, DT, NN, JJ, NN, IN, NN, NNS, .]</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>[24125, 6952, 5610, 5157, 2551, 1506, 25443, 1...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sentence: 10000</td>\n",
       "      <td>[U.N., relief, coordinator, Jan, Egeland, said...</td>\n",
       "      <td>[NNP, NN, NN, NNP, NNP, VBD, NNP, ,, NNP, ,, J...</td>\n",
       "      <td>[B-geo, O, O, B-per, I-per, O, B-tim, O, B-geo...</td>\n",
       "      <td>[24685, 1596, 3575, 15939, 34577, 5674, 7886, ...</td>\n",
       "      <td>[7, 1, 1, 12, 2, 1, 4, 1, 7, 1, 16, 1, 16, 1, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Sentence #                                               Word  \\\n",
       "0      Sentence: 1  [Thousands, of, demonstrators, have, marched, ...   \n",
       "1     Sentence: 10  [Iranian, officials, say, they, expect, to, ge...   \n",
       "2    Sentence: 100  [Helicopter, gunships, Saturday, pounded, mili...   \n",
       "3   Sentence: 1000  [They, left, after, a, tense, hour-long, stand...   \n",
       "4  Sentence: 10000  [U.N., relief, coordinator, Jan, Egeland, said...   \n",
       "\n",
       "                                                 POS  \\\n",
       "0  [NNS, IN, NNS, VBP, VBN, IN, NNP, TO, VB, DT, ...   \n",
       "1  [JJ, NNS, VBP, PRP, VBP, TO, VB, NN, TO, JJ, J...   \n",
       "2  [NN, NNS, NNP, VBD, JJ, NNS, IN, DT, NNP, JJ, ...   \n",
       "3     [PRP, VBD, IN, DT, NN, JJ, NN, IN, NN, NNS, .]   \n",
       "4  [NNP, NN, NN, NNP, NNP, VBD, NNP, ,, NNP, ,, J...   \n",
       "\n",
       "                                                 Tag  \\\n",
       "0  [O, O, O, O, O, O, B-geo, O, O, O, O, O, B-geo...   \n",
       "1  [B-gpe, O, O, O, O, O, O, O, O, O, O, O, O, O,...   \n",
       "2  [O, O, B-tim, O, O, O, O, O, B-geo, O, O, O, O...   \n",
       "3                  [O, O, O, O, O, O, O, O, O, O, O]   \n",
       "4  [B-geo, O, O, B-per, I-per, O, B-tim, O, B-geo...   \n",
       "\n",
       "                                            Word_idx  \\\n",
       "0  [17755, 29027, 34706, 11443, 30908, 10389, 334...   \n",
       "1  [26574, 19104, 22117, 25087, 22100, 12616, 219...   \n",
       "2  [8087, 34712, 15377, 2082, 32462, 24997, 13078...   \n",
       "3  [24125, 6952, 5610, 5157, 2551, 1506, 25443, 1...   \n",
       "4  [24685, 1596, 3575, 15939, 34577, 5674, 7886, ...   \n",
       "\n",
       "                                             Tag_idx  \n",
       "0  [1, 1, 1, 1, 1, 1, 7, 1, 1, 1, 1, 1, 7, 1, 1, ...  \n",
       "1  [16, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...  \n",
       "2  [1, 1, 4, 1, 1, 1, 1, 1, 7, 1, 1, 1, 1, 1, 15,...  \n",
       "3                  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]  \n",
       "4  [7, 1, 1, 12, 2, 1, 4, 1, 7, 1, 16, 1, 16, 1, ...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_group = data.groupby('Sentence #', as_index=False)['Word','POS','Tag','Word_idx','Tag_idx'].agg(lambda x : list(x))\n",
    "data_group.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pad the sequences to be of the same length\n",
    "\n",
    "Keras expects all the sequences of words/tags to be of the same length. So we'll identify the longest sequence and pad all the rest of the sequences to be of that length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length of sequences : 104\n"
     ]
    }
   ],
   "source": [
    "# Calculate the vocabulary size\n",
    "n_tokens = len(set(data['Word']))\n",
    "\n",
    "# Calculate the number of unique NER tokens\n",
    "n_tags = len(set(data['Tag']))\n",
    "\n",
    "# Calculate the length of the longest sequence\n",
    "max_length = max([len(x) for x in data_group['Word_idx']])\n",
    "\n",
    "print(f\"Max length of sequences : {max_length}\")\n",
    "\n",
    "# Pad the tokens\n",
    "pad_tokens = pad_sequences(data_group['Word_idx'],maxlen=max_length, padding='post', value = n_tokens - 1)\n",
    "\n",
    "# Pad the tags\n",
    "pad_tags = pad_sequences(data_group['Tag_idx'],maxlen=max_length, padding='post', value = tag2idx[\"O\"])\n",
    "\n",
    "# One-hot encode the tags\n",
    "encoded_tags = [to_categorical(i , num_classes=n_tags) for i in pad_tags]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let us see what is the effect of these transformations on the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original words sequence : \n",
      "[17755, 29027, 34706, 11443, 30908, 10389, 33495, 12616, 2825, 23743, 6437, 13078, 13876, 4854, 13298, 23743, 8528, 29027, 18277, 5011, 31552, 26038, 20246, 16765]\n",
      "\n",
      "Padded words sequence : \n",
      "[17755 29027 34706 11443 30908 10389 33495 12616  2825 23743  6437 13078\n",
      " 13876  4854 13298 23743  8528 29027 18277  5011 31552 26038 20246 16765\n",
      " 35177 35177 35177 35177 35177 35177 35177 35177 35177 35177 35177 35177\n",
      " 35177 35177 35177 35177 35177 35177 35177 35177 35177 35177 35177 35177\n",
      " 35177 35177 35177 35177 35177 35177 35177 35177 35177 35177 35177 35177\n",
      " 35177 35177 35177 35177 35177 35177 35177 35177 35177 35177 35177 35177\n",
      " 35177 35177 35177 35177 35177 35177 35177 35177 35177 35177 35177 35177\n",
      " 35177 35177 35177 35177 35177 35177 35177 35177 35177 35177 35177 35177\n",
      " 35177 35177 35177 35177 35177 35177 35177 35177]\n",
      "\n",
      "Original tags sequence : \n",
      "[1, 1, 1, 1, 1, 1, 7, 1, 1, 1, 1, 1, 7, 1, 1, 1, 1, 1, 16, 1, 1, 1, 1, 1]\n",
      "\n",
      "Padded tags sequence : \n",
      "[ 1  1  1  1  1  1  7  1  1  1  1  1  7  1  1  1  1  1 16  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1]\n",
      "\n",
      "\n",
      "Encoded tags : \n",
      "[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Original words sequence : \\n{data_group['Word_idx'][0]}\")\n",
    "print(f\"\\nPadded words sequence : \\n{pad_tokens[0]}\")\n",
    "\n",
    "print(f\"\\nOriginal tags sequence : \\n{data_group['Tag_idx'][0]}\")\n",
    "print(f\"\\nPadded tags sequence : \\n{pad_tags[0]}\")\n",
    "\n",
    "print(\"\\n\\nEncoded tags : \")\n",
    "for i in encoded_tags[0][:10]:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split into Train, Validation and Test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape : (32372, 104) \n",
      "y_train shape : (32372, 104, 17)\n",
      "\n",
      "X_val shape   : (10791, 104) \n",
      "y_val shape   : (10791, 104, 17)\n"
     ]
    }
   ],
   "source": [
    "# Keep aside 10% data as test set\n",
    "tokens, X_test, tags, y_test = train_test_split(pad_tokens, encoded_tags, test_size=0.1, random_state = 100)\n",
    "\n",
    "# Split remaining 90% data into train and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(tokens, tags, test_size=0.25, random_state = 100)\n",
    "\n",
    "y_train = np.array(y_train)\n",
    "y_val = np.array(y_val)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "print(f\"X_train shape : {X_train.shape} \\ny_train shape : {y_train.shape}\")\n",
    "print(f\"\\nX_val shape   : {X_val.shape} \\ny_val shape   : {y_val.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the Bidirectional-LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Bidirectional, LSTM, Dropout, Dense, Embedding\n",
    "from tensorflow.keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the random seed for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the input and output dimensions for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_dim : 35179 \n",
      "output_dim : 64\n",
      "input_length : 104\n",
      "number of tags : 17\n"
     ]
    }
   ],
   "source": [
    "input_dim = len(word2idx) + 1\n",
    "output_dim = 64 # creating word vectors of length 64\n",
    "input_length = max([len(s) for s in data_group['Word_idx']])\n",
    "n_tags = len(tag2idx)\n",
    "\n",
    "print(f\"input_dim : {input_dim} \\noutput_dim : {output_dim}\\ninput_length : {input_length}\\nnumber of tags : {n_tags}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_9 (Embedding)      (None, 104, 64)           2251456   \n",
      "_________________________________________________________________\n",
      "bidirectional_9 (Bidirection (None, 104, 128)          66048     \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 104, 17)           2193      \n",
      "=================================================================\n",
      "Total params: 2,319,697\n",
      "Trainable params: 2,319,697\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(input_dim = input_dim,\n",
    "                    output_dim = output_dim,\n",
    "                    input_length = input_length))\n",
    "\n",
    "model.add(Bidirectional(LSTM(units = output_dim, \n",
    "                             return_sequences = True,\n",
    "                             dropout = 0.2,\n",
    "                             recurrent_dropout = 0.2)))\n",
    "\n",
    "model.add(Dense(n_tags, activation = 'softmax'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32372 samples, validate on 10791 samples\n",
      "Epoch 1/10\n",
      "32372/32372 [==============================] - 24s 754us/sample - loss: 0.6710 - accuracy: 0.9517 - val_loss: 0.1802 - val_accuracy: 0.9677\n",
      "Epoch 2/10\n",
      "32372/32372 [==============================] - 19s 602us/sample - loss: 0.1627 - accuracy: 0.9679 - val_loss: 0.1511 - val_accuracy: 0.9677\n",
      "Epoch 3/10\n",
      "32372/32372 [==============================] - 19s 602us/sample - loss: 0.1393 - accuracy: 0.9679 - val_loss: 0.1249 - val_accuracy: 0.9677\n",
      "Epoch 4/10\n",
      "32372/32372 [==============================] - 19s 602us/sample - loss: 0.1091 - accuracy: 0.9694 - val_loss: 0.0963 - val_accuracy: 0.9723\n",
      "Epoch 5/10\n",
      "32372/32372 [==============================] - 19s 600us/sample - loss: 0.0877 - accuracy: 0.9737 - val_loss: 0.0835 - val_accuracy: 0.9749\n",
      "Epoch 6/10\n",
      "32372/32372 [==============================] - 19s 600us/sample - loss: 0.0773 - accuracy: 0.9760 - val_loss: 0.0760 - val_accuracy: 0.9770\n",
      "Epoch 7/10\n",
      "32372/32372 [==============================] - 19s 601us/sample - loss: 0.0694 - accuracy: 0.9781 - val_loss: 0.0689 - val_accuracy: 0.9789\n",
      "Epoch 8/10\n",
      "32372/32372 [==============================] - 19s 599us/sample - loss: 0.0612 - accuracy: 0.9817 - val_loss: 0.0605 - val_accuracy: 0.9836\n",
      "Epoch 9/10\n",
      "32372/32372 [==============================] - 19s 596us/sample - loss: 0.0520 - accuracy: 0.9860 - val_loss: 0.0516 - val_accuracy: 0.9869\n",
      "Epoch 10/10\n",
      "32372/32372 [==============================] - 19s 599us/sample - loss: 0.0433 - accuracy: 0.9890 - val_loss: 0.0445 - val_accuracy: 0.9890\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, np.array(y_train),\n",
    "                    epochs = 10,\n",
    "                    batch_size=512,\n",
    "                    validation_data = (X_val, y_val),\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Human annotation quality is around 98%. We can see that using advanced ML techniques we can get around 99% accuracy on unseen data which is phenomenal :). Hope you enjoyed this article."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
